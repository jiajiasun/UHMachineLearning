{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Classifying seismic P-wave receiver functions using Ensemble Learning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The goal of this lab exercise is to get students familiar with the Scikit-Learn implementation of **Ensemble Learning** methods, in  particular, the **boosting** methods. \n\nSimilar to previous two lab exercise, your task is to classify the P-wave receiver functions, which were computed based on the recorded seismic data, into two categories: good and bad. The entire data set consists of 12,597 receiver functions (i.e., seismic traces), each of which was visually examined and manually labeled as either good or bad by one of Prof. Aibing Li's PhD students, Ying Zhang, in the Department of Earth and Atmospheric Sciences at University of Houston. The good seismic traces are labled (or, encoded) as 1, and bad seismic traces are encoded as 0. <br>\n\nAfter finishing this exercise, you can expect to <br>\n1. be able to implement AdaBoost and Gradient Boosting using Scikit-Learn; and <br>\n2. be able to fune tune the hyperparameters such as **max_depth**, **n_estimators** and **learning_rate**. <br>\n\n<br>\nAuthor: Jiajia Sun @ University of Houston, 03/07/2019"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Import data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport h5py\nwith h5py.File(\"../Traces_qc.mat\") as f:\n    ampdata = [f[element[0]][:] for element in f[\"Data\"][\"amps\"]]\n    flag = [f[element[0]][:] for element in f[\"Data\"][\"Flags\"]]\n    ntr = [f[element[0]][:] for element in f[\"Data\"][\"ntr\"]]\n    time = [f[element[0]][:] for element in f[\"Data\"][\"time\"]]\n    staname = [f[element[0]][:] for element in f[\"Data\"][\"staname\"]]\n    \nampall = np.zeros((1,651))\nflagall = np.zeros(1)\nfor i in np.arange(201):\n    ampall = np.vstack((ampall, ampdata[i]))\n    flagall = np.vstack((flagall, flag[i]))\namp_data = np.delete(ampall, 0, 0)\nlabel_data = np.delete(flagall, 0, 0)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.random.seed(42)\nall_data = np.append(amp_data,label_data,1) # put all the seismic traces and their lables into one matrix.",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we randomly permute the data (i.e., the rows in the matrix *all_data* that was just construted above)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "all_data_permute = all_data[np.random.permutation(all_data.shape[0]),:] ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similar to previous lab exercises, we split the whole data set into training and validation sets. BTW, in Scikit-Learn, there is a module called [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) that can automatically split the dataset for you. But for this exercise, we are not going to use that. To keep things consistent, we do the splitting in the same way as in previous lab exercises."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train = all_data_permute[:10000,:-1]\ny_train = all_data_permute[:10000,-1]\n\nX_validation = all_data_permute[10000:,:-1]\ny_validation = all_data_permute[10000:,-1]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train an AdaBoostClassifier"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To learn more about AdaBoostClassifier as implemented in Scikit-Learn, please click [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). <br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Standard AdaBoostClassifier\nOne thing you need to specify in order to run AdaBoostClassifier is the base estimator. If you forget what a base estimator is, please refer to Slide 52. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us import AdaBoostClassifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import AdaBoostClassifier",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For illustration purposes, I am going to use [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) as my base estimator. The number of Decision Trees in my ensemble is 200. Again, this is only for illustrating how to set up **AdaBoostClassifier**. Please feel free to use any other classification algorithm. <br>\n<br>\nBecause I decide to use **DecisionTreeClassifier** as my base estimator, I also need to import **DecisionTreeClassifier** from sklearn."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next line of code shows how I set up the **AdaBoostClassifier**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rf_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), n_estimators=100, learning_rate = 0.5, random_state = 42)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As always, please set your *random_state* = 42, so that whenven random sampling is involved, we all get the same sampling results."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One more thing before we proceed to train an **AdaBoostClassifier**. That is, learning rate. For **AdaBoost**, remember that at each iteration we adaptively adjust the weights on each sample, depending on how well they were classified by the previous predictor. The learning rate decides how much adjusment is going to happen to the samples. <br>\n<br>\nFor example, the following figure shows the decision boundaries of five consecutive predictors on the [moons](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) dataset. In the left figure, the decision boundaries learned from these five consecutive predictors are indicated by the integer numbers. We observe that, the first classifier gets many instances wrongly labeled. Then, at the next iteration, the weights of these misclassified lables get boosted. The second classifier, therefore, ends up doing a better job classifying these instances. This process repeats until all the **n_estimators** are included in the ensemble. The plot on the right represents the same sequence but with learning rate halved. That is, the weights on the misclassified instances only get boosted half as much at every iteration. That is why you observe that the decision boundaries learned with **learning_rate** = 0.5 are updated more slowly.<br> \n\n<img src = \"Learning_Rate.png\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we are ready to train an AdaBoostClassifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rf_clf.fit(X_train, y_train)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "AdaBoostClassifier(algorithm='SAMME.R',\n          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=0.5, n_estimators=100, random_state=42)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, make predictions on the validation dataset using the AdaBoostClassifier that you just trained above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = rf_clf.predict(X_validation)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As usual, print out the classification report."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report\nprint(classification_report(y_validation, y_pred, target_names=['bad','good']))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "             precision    recall  f1-score   support\n\n        bad       0.93      0.94      0.93      1994\n       good       0.79      0.75      0.77       603\n\navg / total       0.90      0.90      0.90      2597\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can calculate the prediction error on training data as follows."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "1 - rf_clf.score(X_train, y_train)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "0.04920000000000002"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similarly, the prediction error on validation data can be calculated as follow."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "1 - rf_clf.score(X_validation,y_validation)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "0.10319599537928381"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### AdaBoostClassifier with balanced class_weight"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Alternatively, you can train another AdaBoostClassifier by setting **class_weight** to **'balanced'**. You did something similar in previous lab exercise for Random Forests. This parameter can be used to balance the training samples for which the number of samples in each classe is very different from each other."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rf_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2, class_weight = 'balanced'), n_estimators=100, learning_rate = 0.5, random_state = 42)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "rf_clf.fit(X_train, y_train)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "AdaBoostClassifier(algorithm='SAMME.R',\n          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=0.5, n_estimators=100, random_state=42)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = rf_clf.predict(X_validation)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(classification_report(y_validation, y_pred, target_names=['bad','good']))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "             precision    recall  f1-score   support\n\n        bad       0.95      0.90      0.93      1994\n       good       0.72      0.85      0.78       603\n\navg / total       0.90      0.89      0.89      2597\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The following code is based on a modification of the codes in this webpage\n# http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nimportances = rf_clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(10):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(50), importances[indices][:50])\nplt.xticks(range(50), indices, rotation = 90)\nplt.xlim([-1, 50])\nplt.tight_layout()\nplt.show()",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Feature ranking:\n1. feature 49 (0.053462)\n2. feature 50 (0.038199)\n3. feature 54 (0.019763)\n4. feature 51 (0.016490)\n5. feature 99 (0.015265)\n6. feature 62 (0.015223)\n7. feature 47 (0.014685)\n8. feature 48 (0.014045)\n9. feature 53 (0.014040)\n10. feature 155 (0.013670)\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7fd30c789080>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The prediction error on training data\n1 - rf_clf.score(X_train, y_train)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "0.06489999999999996"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The prediction error on validation data\n1 - rf_clf.score(X_validation,y_validation)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "0.11166730843280703"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train a GradientBoostingClassifier\nTo learn more about how GradientBoostingClassifier is implemented in Scikit-Learn, please click [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The first thing to do is import **GradientBoostingClassifier** from Scikit-Learn."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, set up **GradientBoostingClassifier** by assigning values to the hyperparameters such as max_depth, n_estimators, learning_rate, etc. **Note:** For gradient boosting, you do not need to choose the base estimator, beccause the base estimator has been defaulted to **Decision Trees**. Also, for **GradientBoostingClassifier**, there is no **class_weight**.<br>\n<br>\nFor **GradientBoostingClassifier**, the learning_rate scales (more precisely, shrinks) the contribution of each tree. If **learning_rate** is too small, such as 0.1, you will need many trees in the ensemble to fit the training data. The following figure shows two ensembles trained by **GradientBoostingRegressor** with **learning_rate** = 0.1. The one on the left does not have enough trees to fit the training data, while the one on the right has too many trees and overfits the training data. <br>\n\n<img src = \"GBlearning.PNG\">\n\nTo give you a better understanding of **learning_rate**, the following example compares the predictors learned by **GradientBoostingRegressor** with two different learning rates. \n<br>\n\n<img src = \"GBlearning2.png\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With a good understanding of these hyperparameters, now we are ready to set up our **GradientBoostingClassifier**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gtb = GradientBoostingClassifier(max_depth = 2, n_estimators = 200, learning_rate = 0.5, random_state=42)",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gtb.fit(X_train, y_train)",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.5, loss='deviance', max_depth=2,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=200,\n              presort='auto', random_state=42, subsample=1.0, verbose=0,\n              warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After training is done, we need to use the trained predictor (or, classifier) to make predictions on our validation dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = gtb.predict(X_validation)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we can print out the classification report."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(classification_report(y_validation, y_pred, target_names=['bad','good']))",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "             precision    recall  f1-score   support\n\n        bad       0.92      0.94      0.93      1994\n       good       0.79      0.74      0.76       603\n\navg / total       0.89      0.89      0.89      2597\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The prediction error on training data\n1 - gtb.score(X_train, y_train)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "0.01880000000000004"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The prediction error on validation data\n1 - gtb.score(X_validation,y_validation)",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "0.10589141316904116"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task\nThe prediction errors on validation dataset that I obtained are: 0.10319599537928381 for standard **AdaBoostClassifier**, 0.11166730843280703 for **AdaBoostClassifier** with **balanced class_weight**, and 0.10589141316904116 for **GradientBoostingClassifier**. So, the lowest error that I achieved is 0.10319599537928381, which corresponds to a prediction accuracy of 1-0.10319599537928381 = 0.8968. <br>\n\n<font color = red>**Your task**</font>: do whatever you can to come up with a predictor (or synonymously, classifier) with better predication accuracy. By 'better', I mean better than 0.8968. <font color = red>**(100 points)**</font> <br>\n<br>\n**HINT:** Feel free to try anything you can think of. For example, it is up to you to choose between **AdaBoostClassifier** and **GradientBoostingClassifier**. Or, if you like, you can even create your own ensemble by combining, say, support vector machine, logistic regression and decision trees. Also, feel free to try different values for the hyperparameters such as **max_depth**, **n_estimators**, **learning_rate**, and **class_weight** (not available for Gradient Boosting)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: Please write down your code below. <font color=red>**DO NOT**</font> change the above codes."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Bonus\nIf you achieve a prediction accuracy of **91% or above**, you get <font color = red>**10 bonus points**</font>. <br>\n<br>\nIf you achieve a prediction accuracy of **92% or above**, you will get a <font color = red>**$20 gift card**</font> from a merchant of your choice. (Limited to the first five submissions) <br>\n<br>\nIf you achieve a prediction accuracy of **93% or above**, I will buy you <font color = red>**a dinner at Eric's**</font>."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Acknowledgments\nI would like to thank Ying Zhang for manually labeling all the seismic traces, and Prof. Aibing Li for making this data set available to the students in this class. I also want to thank Dr. Tianyuan Guan for sponsoring this event (i.e., the gift cards and dinners)! <br>\n\n<img src = \"photo.png\">"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}