{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Applying deep learning to the automatic classification of seismic traces"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Hello, eveyone! Welcome to the lab on deep learning! Deep learning has achieved unprecedented success in many areas of studies and resulted in a large amount of AI-driven products. <br>\n<br>\nIn this lab exercise, you are going to use [TensorFlow](https://www.tensorflow.org/) to train a deep neural network that can automatically classify the seismic P-wave receiver functions into two categories: good and bad. You will see that with TensorFlow, it is fairly straightforward to implement deep learning, and with deep learning, it is fairly easy to echieve a prediction accuracy of ~91%. <br>\n<br>\nAfter finishing this lab exercise, you can expect to be able to:<br>\n1. use TensorFlow to implement deep learning;\n2. train your first deep learning model for classification of seismic traces; <br>\n<br>\n\nAuthor: Jiajia Sun at University of Houston, 04/16/2019"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Import and prepare data\nThe way to import the seismic data is pretty the same as before. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport h5py\nwith h5py.File(\"../Traces_qc.mat\") as f:\n    ampdata = [f[element[0]][:] for element in f[\"Data\"][\"amps\"]]\n    flag = [f[element[0]][:] for element in f[\"Data\"][\"Flags\"]]\n    ntr = [f[element[0]][:] for element in f[\"Data\"][\"ntr\"]]\n    time = [f[element[0]][:] for element in f[\"Data\"][\"time\"]]\n    staname = [f[element[0]][:] for element in f[\"Data\"][\"staname\"]]\n    \nampall = np.zeros((1,651))\nflagall = np.zeros(1)\nfor i in np.arange(201):\n    ampall = np.vstack((ampall, ampdata[i]))\n    flagall = np.vstack((flagall, flag[i]))\namp_data = np.delete(ampall, 0, 0)\nlabel_data = np.delete(flagall, 0, 0)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.random.seed(42)\nall_data = np.append(amp_data,label_data,1) # put all the seismic traces and their lables into one matrix.",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we need to prepare the data properly."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "all_data_permute = all_data[np.random.permutation(all_data.shape[0]),:] ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train = all_data_permute[:10000,:-1]\ny_train = all_data_permute[:10000,-1]\n\nX_validation = all_data_permute[10000:,:-1]\ny_validation = all_data_permute[10000:,-1]",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The following code performs one hot encoding.\ntmpa = np.zeros((y_train.shape[0],2))\ntmpa[np.arange(y_train.shape[0]), y_train.astype(int)] = 1\ny_train_new = tmpa\n\ntmpb = np.zeros((y_validation.shape[0],2))\ntmpb[np.arange(y_validation.shape[0]), y_validation.astype(int)] = 1\ny_validation_new = tmpb",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that, in the above cell, I transformed the **y_train** and **y_validation** to **y_train_new** and **y_validation_new**. The reason is that, our output layer will consist of 2 neurons. Therefore, we need to convert 0, that is, the categorical number representing bad seismic traces, to a 1D array [1,0] Similarly, we convert 1, i.e., the categorical number representing the good seismic traces, to a 1D array [0,1]. <br>\n<br>\nThis is called '**one hot encoding**' in machine learning community. This is very commonly adopted practice for deep learning when it comes to classification problems. If you want to learn more about one hot encoding, you can simply search 'one hot encoding' in Google, or go to this Scikit-Learn [webpage](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). <br>\n<br>\nFor the purpose of this exercise, please do not worry too much about it. The simplest way to understand one hot encoding is that, we represent our categories (or, different classes), which are the expected outcomes of a deep neural network, using some sort of coding. For example, suppose we have four classes to predict. Then, using 'one hot encoding', we would encode our first class as [1,0,0,0], the second class as [0,1,0,0], the third class as [0,0,1,0], and the fourth class as [0,0,0,1]. <br>\n<br>\nOne thing that you do need to pay close attention is that, your training data are now **[X_train, <font color = red>y_train_new</font>]**, and your validation data are now **[X_validation, <font color = red>y_validation_new</font>]**."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Create the computation graph"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 1:**</font> Explain what a compuation graph is in the context of TensorFlow and deep neural networks. <font color = red>**(5 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 1:\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The first thing to do is to import TensorFlow into our workplace."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import tensorflow as tf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us define our neural networks. We will be using a neural network with two hidden layers and one output layer. Specifically, there are 300 neurons in the first hidden layer, 100 neurons in the second hidden layer and 2 neurons in the output layer. <br>\n<br>\nHow many neurons should we have for our input layer? <br> \n<font color=red>**651**</font>!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 2:**</font> Please explain why we are having 651 neurons for our input layer. <font color = red>**(5 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Ansewr to Task 2:\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "n_inputs = 651\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 3:**</font> Create two placeholders, one for X, the other for y. The placeholder X will hold the seismic amplitude values from each seismic trace and the placeholder y will hold the labels for all these seismic traces. <font color = red>**(10 points)**</font> <br>\n<br>\n**HINT:** Since we do not know exactly how many seismic traces will be fed into our neural networks, the shape of X could be *(None, n_inputs)* where None simply means that we do not know how many instances are coming at the moment. Now, for the shape of y, since we have used one-hot-encoding, its shape should be *(None, 2)*. And since we are using one-hot-encoding, its data type should be tf.float32. To learn more about tf.placehoder, please see this [webpage](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 3\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 4:**</font> Create the two hidden layers as well as the output layer using *tf.layers.dense*. <font color = red>**(15 points)**</font> <br>\n<br>\n**HINT:** If you forget how to use *tf.layers.dense*, please refer to the accompanying notework entitled 'TF_DNN_example.ipynb'."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 4\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we need to define our cost function. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 5:**</font> Define the cost function. <font color = red>**(15 points)**</font> <br>\n<br>\n**HINT:** Because of the use of one-hot-encoding, you should use *[softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)* to define your cost function, as opposed to *[sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)*. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 5\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 6:**</font> Define the optimization scheme. We are going to use GradientDescentOptimizer with a learning rate of 0.1, and we want to minimize the cost function that was defined previously. <font color = red>**(15 points)**</font> <br>\n<br>\n**HINT:** This can be done by defining *optimizer* and *training*, the same way as in the accompanying notebook entitled 'TF_DNN_example.ipynb'."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 6\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, define how the predications are going to be evaluated."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "compare = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(compare,tf.float32))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Execute the compuation graph"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 7:**</font> Define a global variable initializer that will intialize all the variables automatically for you once a TensorFlow session is run. Please name your initializer *init*. <font color = red>**(5 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 7\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "saver = tf.train.Saver()  # This is to save the model that you are going to train.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to use mini-Batch Gradient Descent to train our neural networks. Therefore, we need to define our mini-batch size as well as how many epochs we would like to have. If you find these terminologies unfamiliar, please refer back to our lecture on gradient descent."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "n_epochs = 30\nbatch_size = 50\nn_batches = int(np.ceil(10000/batch_size))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we define how to fetch the mini-batch from our training dataset. Note that, in the example notebook 'TF_DNN_example.ipynb', I used **.train.next_batch** to fetch the mini-batch from MNIST training dataset. However, this function is not generally applicable. So, we need to define our own way of fetching mini-batch."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def fetch_batch(epoch, batch_index, batch_size):\n    np.random.seed(epoch * n_batches + batch_index) \n    indices = np.random.randint(10000, size=batch_size)  \n    X_batch = X_train[indices] \n    y_batch = y_train_new[indices,:]\n    return X_batch, y_batch",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Task 8: Open a new tf.Session (5 ponints)\nwith \n    # Task 9: Initialize all the variables (5 points)\n    \n    for epoch in np.arange(n_epochs):\n        for batch_index in np.arange(n_batches):\n            # Task 10: fetch the mini-batch using the fetch_batch function defined above (5 points)\n\n            # Task 11: run the training using the mini-batch that you just fetched. (5 points)\n\n        # Task 12: compute the predictioin accuracies on both training and validation data sets, and name them as \n        # accuracy_train and accuracy_test, respectively. (10 points) \n        # HINT: pay close attention to what you feed to the calucation of prediction accuracies on validation dataset.\n        \n        \n        print(epoch, \"Train accuracy:\", accuracy_train, \"Test accuracy:\", accuracy_test)\n        if epoch == 1:\n            plt.plot(epoch, accuracy_train,'-ro',label='accuracy_train')\n            plt.plot(epoch, accuracy_test,'-bo',label='accuracy_test')\n        else:\n            plt.plot(epoch, accuracy_train,'-ro')\n            plt.plot(epoch, accuracy_test,'-bo')\n    \n    plt.legend(loc=\"lower right\", fontsize=16)\n    plt.show()\n    \n    save_patch = saver.save(sess,'./my_DNN_model.ckpt')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4. Make predictions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us compare the predictions on some of the seismic traces in the validation dataset with their true labels. Note: you do not need to do anything. Just clik the cells and run them."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_DNN_model.ckpt\") # or better, use save_path\n    X_new_scaled = X_validation[:30,:]\n    Z = output.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Predicted classes:\", y_pred)\nprint(\"Actual classes:   \", y_validation.astype(int)[:30])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5. Visualize the computation graph "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us visualize the computation graph that you created before. Note: you do not need to do anything below. Simply, click the cell, and run it."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# the following code is copied and pasted from Aurelien Geron's book. This code was orginally writeen by A. Mordvintsev.\nfrom IPython.display import clear_output, Image, display, HTML\n\ndef strip_consts(graph_def, max_const_size=32):\n    \"\"\"Strip large constant values from graph_def.\"\"\"\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add() \n        n.MergeFrom(n0)\n        if n.op == 'Const':\n            tensor = n.attr['value'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n    return strip_def\n\ndef show_graph(graph_def, max_const_size=32):\n    \"\"\"Visualize TensorFlow graph.\"\"\"\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    code = \"\"\"\n        <script>\n          function load() {{\n            document.getElementById(\"{id}\").pbtxt = {data};\n          }}\n        </script>\n        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n        <div style=\"height:600px\">\n          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n        </div>\n    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n\n    iframe = \"\"\"\n        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n    \"\"\".format(code.replace('\"', '&quot;'))\n    display(HTML(iframe))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "show_graph(tf.get_default_graph())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## BONUS \n<font color = red>(**10 points**):</font> Now that you know how to implement deep learning using TensorFlow, try to change the network structure (e.g., the number of layers, and the number of neurons in each layer), and see if you can achieve a prediction accuracy of >92%. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(answer to Bonus:)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Acknowledgments\nI would like to thank Ying Zhang for manually labeling all the seismic traces, and Prof. Aibing Li for making this data set available to the students in this class. <br>\n\n<img src = \"photo.png\" width=\"400\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Congratulations! You have now become one of those people who know how to use TensorFlow to do deep learning!"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}