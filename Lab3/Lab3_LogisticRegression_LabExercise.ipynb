{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Classifying seismic receiver functions using logistic regression"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this lab exercise, your task is to classify seismic receiver functions into two categories: good and bad. The bad seismic traces, in practice, are excluded from all further analysis. And only good seismic traces are kept for the subsequent quantitative analysis. To perform classification, you will implement logistic regression using Scikit-learn package. Specifically, you are going to use the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module by importing it from [Scikit-Learn](https://scikit-learn.org/stable/). <br>\n<br>\nAfter finishing this exercise, you will understand: <br>\n- How to implement logistic regression using Scikit-Learn; <br>\n- The typical data preprocessing steps involved in many machine learning implementation; <br>\n- How to evaluate a learned model; <br>\n- How the learning errors of training and validation sets change as the size of training data set increases. <br>\n<br>\n\nYou will also see that machine learning can significantly accelerate the classification problem from manual labeling that takes weeks to automatic labeling that takes about 20 seconds. <br>\n<br>\nAuthor: Jiajia Sun, 02/07/2019 at University of Houston."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1. Introduction to USArray data\nThe seismic data we are going to use for this lab exercise was recorded by USArray Transportable Array (TA). The TA has traversed the continental United States and collected voluminous amounts of broadband seismic data that contain extremely rich information for mapping the structures of the Earth’s interior underneath North America. It is currently being deployed in Alaska.\n\nHere is a picutre summarizing the locations of current USArray TA seismic receivers.\n<img src=\"TA_AK.png\">\n\nThe data we are going to classify is actually the P-wave receiver functions that were computed based on raw seismic data. P-wave receiver functions are widely used in crustal studies, because they provide important information about the crustal thickness. The seismological data used in our study are from earthquakes with a distance range of $30^\\circ$ to $90^\\circ$ and a magnitude of Mb 5 and above recorded at 201 stations in Alaska from the TA and Alaska Regional Network. 12,597 receiver function traces were obtained, and manually labeled ‘good’ or ‘bad’.\n\nTo learn more about USArray data, please refer to the following resources: <br>\n1\\. http://www.usarray.org/researchers/dataas  <br>\n2\\. http://ds.iris.edu/ds/nodes/dmc/earthscope/usarray/ <br>\n3\\. http://www.usarray.org/Alaska <br>\n<br>\nFor more information on receiver functions, please refer to the following materials: <br>\n1\\. https://ds.iris.edu/media/workshop/2013/01/advanced-studies-institute-on-seismological-research/files/lecture_introrecf.pdf <br>\n2\\. http://www.diss.fu-berlin.de/diss/servlets/MCRFileNodeServlet/FUDISS_derivate_000000001205/3_Chapter3.pdf?hosts= <br>\n<br>\nNote that, for the purpose of this class, you do not need to know anything about seismology or receiver functions."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2. Import data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following code imports the data from Traces_qc.mat."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport h5py\nwith h5py.File(\"../Traces_qc.mat\") as f:\n    ampdata = [f[element[0]][:] for element in f[\"Data\"][\"amps\"]]\n    flag = [f[element[0]][:] for element in f[\"Data\"][\"Flags\"]]\n    ntr = [f[element[0]][:] for element in f[\"Data\"][\"ntr\"]]\n    time = [f[element[0]][:] for element in f[\"Data\"][\"time\"]]\n    staname = [f[element[0]][:] for element in f[\"Data\"][\"staname\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ampall = np.zeros((1,651))\nflagall = np.zeros(1)\nfor i in np.arange(201):\n    ampall = np.vstack((ampall, ampdata[i]))\n    flagall = np.vstack((flagall, flag[i]))\namp_data = np.delete(ampall, 0, 0)\nflag_data = np.delete(flagall, 0, 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The **amp_data** stores the seismic amplitudes from all seismic stations. The **flag_data** contains the labels for each seismic traces. These labels are encoded as 1s and 0s, with 0 representing bad seismic traces, and 1 corresponding good seismic traces. Recall that, logistic regression is a supervised machine learning, and therefore, it requires labels. Also recall that, logistic regression is a binary classification algorithm, and the labels are most often expressed numerically as 0s and 1s. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us plot up a few 'good' seismic traces which are all labeled as 1s."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "goodtraceindex = np.nonzero(flag_data)[0].reshape(-1,1)\n# plot a few good traces (before scaling is applied)\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1,5, figsize=(15, 6), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace = .5, wspace=.001)\nfig.suptitle('a few good traces', fontsize=20)\n\naxs = axs.ravel()\nic = 0\nfor icount in goodtraceindex[:5,0]:       \n              \n    axs[ic].plot(amp_data[icount,:], time[0])\n    axs[ic].invert_yaxis()\n    axs[ic].set_xlabel('amplitude')\n    axs[ic].set_ylabel('time')\n        \n    ic = ic + 1\n        \n# tight_layout() will also adjust spacing between subplots to minimize the overlaps    \nplt.tight_layout()      \nplt.show()       ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are a few obvious features that are common to all 'good' traces. First, there is a distinct peak at time 0. Second, there is a second peak around 5 seconds. (This actually provides information about the crustal thickness). Thirdly, the amplitude for the peak at time 0 should be clearly higher than that for the second peak.\n<br>\n\n<font color = red>**Note**</font>: Visualzing data and getting an intuitive understanding of your data is very important. It is almost always done in practice. This is the first step to getting to know your data. Other ways of knowing your data better include summarizing your data using statistics such as max/min, mean, variance, quantile, histogram, etc."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3. Preprocessing (i.e., preparing data for subsequent machine learning)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One common preprocessing step is to normalize your data so that they have a mean of 0 and a standard deviation of 1. The reasons for doing this are twofold. First, for practical machine learning problems, different features have different scales. For example, when it comes to predicting life satisfaction, the GPD per capita might be on the order of ~1000s, whereas the education system might be ranked on a scale of 0 to 1, with 1 representing the best education. It turns out, features with vastly different scales make the optimization biased toward the ones that have large values (e.g., GPD per capita instead of education system). Secondly, for minimization, the shape of the cost function associated with features of different scales becomes elongated, making the gradient descent type of algorithms less efficient. <br>\n<br>\nTherefore, normalizing data is very commonly done in practice."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following code shows how the scaling (or, normalizing) is typically done using Scikit-learn. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(amp_data)\nscaled_ampdata = scaler.transform(amp_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# To find out how many traces are labled as 'bad'\nscaled_ampdata[np.where(flag_data == 0)[0],:].shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# to find out how many traces are labeled as 'good'\nscaled_ampdata[np.nonzero(flag_data)[0],:].shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Total number of seismic traces\nscaled_ampdata.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we need to randomly permute these data before we start doing machine learning. The reason for doing this is to avoid the situation where your training data are ordered in some specific way. For example, it might happen that all the good seismic traces are together, followed by all the bad traces. If we are not careful, our training data set might be all the good seismic traces, and our validation or test data set might be all the bad ones. This is very dangerous because your machine lerning algorithm will not have any chance of learning from the bad seismic traces at the training stage, and you can expect that no matter how you train a machine learning model, it will not predict well on the validation/test data. Randomly permuting the data will ensure that the training set contains data from every category (good and bad), and validation/test set also contains data from all categories. "
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.random.seed(42)\nwhole_data = np.append(scaled_ampdata,flag_data,1) # put all the seismic traces and their lables into one matrix which contain the whole data set for subsequent machine learing.\ntraining_data_permute = whole_data[np.random.permutation(whole_data.shape[0]),:] ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 4. Split the data into training and validation sets\n<font color = red>**Task 1**</font>: Please write a few sentences here to explain why we do this. <font color = red>**(10 points)**</font>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "<answer to Task 1:>\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We have in total 12597 seismic traces. And we are going to use the first 2000 seismic traces and their corresponding labels as our training data set."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train = training_data_permute[0:2000,:-1]\ny_train = training_data_permute[0:2000,-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similarly, we are going to put aside the seismic traces with indices from 10000 to the very end as our validation (or test) data set."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_validation = training_data_permute[10000:,:-1]\ny_validation = training_data_permute[10000:,-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 5. Implementing logistic regression using Scikit-learn"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 2:**</font> Please import the logistic regression module from Scikit-learn. <font color = red>**(10 points)**</font> <br>\n<br>\n**HINT:** If you forget how to do it, please refer back to <fontn color=blue>Lab3_LogisticRegression_example.ipynb</font>."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 3**</font>: Assign the LogisticRegression method to a new variable *log_reg*. <font color = red>**(10 points)**</font> \n\n**HINT**: Please refer back to Lab3_LogisticRegression_example.ipynb. Note that, in Lab3_LogisticRegression_example.ipynb, I set C = 10$^{10}$. However, for this exercise, please use the default value for C that comes with the logistic regression module that you just imported. That is, instead of using LogisticRegression(C=10**10, random_state=42), you should use LogisticRegression(random_state=42)"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 4:**</font> Train a logistic regession model using our training data, i.e., X_train and y_train. <font color = red> **(10 points)**</font>\n\n**HINT:** Again, if you do not know how to do it, please take a look at Lab3_LogisticRegression_example.ipynb. Only one line of code is necessary for this task."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 6. Evaluation of the learned logistic regression model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 5:**</font> Output the accuracy (or score) of the predictions on the **training** data set. <font color = red>**(10 points)**</font>\n\n**HINT**: Please refer to Lab3_LogisticRegression_example.ipynb, if you are not sure what to do."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 6:**</font> Output the accuracy (or score) of the predictions on the **validation** data set. <font color = red>**(10 points)**</font>\n\n**HINT**: Please refer to Lab3_LogisticRegression_example.ipynb, if you are not sure what to do."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 7:**</font> Output the error of the predictions on both the **training** and **validation** data sets. <font color = red>**(5 points)**</font>\n\n**HINT**: error = 1 - accuracy, where accuracy is what you just obtained in Task 5 and 6."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 7. Constructing error curves"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 8:**</font> So far, we have only used 2,000 seismic traces as our training data set. But remember that we can use up to 10,000 traces as our training data set (the remaining 2,597 traces were reserved for validation). For this task, create a training data set with 4000 seismic traces (do not touch the validation data set that we set previously). And \ncompute the errors of the predictions on both training and validation data sets. Similary, create a training data set with 6000, 8000 and 10000 seismic traces, and compute their respective errors on both training and validation data sets. <font color = red>**(30 points)**</font>\n\n**HINT:** To create a training data set with 4000 seismic traces, you can use the following codes: <br>\nX_train = training_data_permute[0:4000,:-1] <br>\ny_train = training_data_permute[0:4000,-1] <br>\n<br>\n**NOTE:** For this task, our validation data set is always the same as before, that is: <br>\nX_validation = training_data_permute[10000:,:-1] <br>\ny_validation = training_data_permute[10000:,-1] <br>\n<br>\nYou do not need to do anything with the validation data set."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 9:**</font> Store the errors of the predictions on **training** data using 2000, 4000, 6000, 8000, and 10000 seismic traces in a Numpy array **train_errors**. Similarly, store the errors of the predictions on **validation** data using 2000, 4000, 6000, 8000, and 10000 seismic traces in a Numpy array **validation_errors**. <font color = red>**(5 points)**</font> <br>\n<br>\n**HINT:** Your **train_errors** should look like this: train_errors = np.array([0.169, 0.17825, 0.17966667, 0.180875, 0.1827]). And your **validation_errors** should look similar (the values in the array might be different though)."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let us plot up the error curves."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\ntrainingsize =  np.array([2000,4000,6000,8000,10000])\nplt.plot(trainingsize,train_errors,'-ro',label=\"training errors\")\nplt.plot(trainingsize,validation_errors,'-bo',label=\"validation errors\")\nplt.title('Learning curves',fontsize=20)\nplt.legend(loc=\"lower right\", fontsize=16)\nplt.xlabel(\"Size of training data\", fontsize=20)\nplt.ylabel(\"Prediction error\", fontsize=20, rotation=90)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**BONUS:** Summarize the change of training and validation errors as the size of the training data increases. Explain it. <font color=red>**(10 points)**</font>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Acknowledgments\nI would like to thank Ying Zhang for manually labeling all the seismic traces, and Prof. Aibing Li for making this data set available to the students in this class. Ms. Zhang also kindly explained the fundamentals of seismic P-wave receiver functions to a non-seismic person (yes, that is me!) <br>\n\n<img src = \"photo.png\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Congratulations! You have now mastered a great skill that allows you to classify things into binary classes using logistic regression. You have also started to use Scikit-learn to do machine learning. You have achieved a lot!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
