{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Automatic classification of seismic P-wave receiver functions using Random Forests (RF)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this lab exercise, we are going to train Random Forests to automatically classify the seismic P-wave receiver functions that were also used in previous two lab exercises, one on logistic regressin and the other one on decision trees.\n\nSpecifically, your task is to classify the P-wave receiver functions, which were computed based on the recorded seismic data, into two categories: good and bad. The entire data set consists of 12,597 receiver functions (i.e., seismic traces), each of which was visually examined and manually labeled as either good or bad by one of Prof. Aibing Li's PhD students, Ying Zhang, in the Department of Earth and Atmospheric Sciences at University of Houston. The good seismic traces are labled (or, encoded) as 1, and bad seismic traces are encoded as 0. <br>\n\nAfter finishing this exercise, you can expect to <br>\n1. be able to implement Random Forests using Scikit-Learn; <br>\n2. better understand the regularization role played by the hyperparameter, **max_depth**; <br>\n3. be able to diagnose overfitting vs. underfitting by constructing the error curves. <br>\n\n<br>\nAuthor: Jiajia Sun @ University of Houston, 02/28/2019"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Review Random Forests\n<font color = red>**Task 1:**</font> Please write one paragraph that summarizes RF (i..e, what is it, and how does it work?) . <font color = red>**(8 points)**</font> <br>\n<br>\n**HINT**: You can include into this paragraph answers to the following questions: <br>\n1. What do 'random' and 'forest' each mean? **(2 points)**<br>\n2. How does RF work? Please refer to Slide 44. **(2 points)** <br>\n3. What are the advantages of RF as compared to Decision Trees? Please feel free to search online for answers. If you do so, please include the webpage's URL in your acknowledgments.**(2 points)** <br>\n4. Anything else you would like to include (e.g., something that you think would help your colleagues understand RF).**(2 points)**<br>\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(Answer to Task 1:)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 2:**</font> Write a short paragraph about the history of RF. Be sure to include into your writing the two terms: **Random Patches** and **Random Subspaces** and what they each mean. <font color = red>**(8 points)**</font> <br>\n<br>\n**HINT**: Please refer back to our lecture slides, in case you need a refresher. <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(Answer to Task 2:)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Import data\n<font color = red>**Task 3:**</font> Import the amplitude data and the labels from *Traces_qc.mat*. Be sure to store the seismic amplitudes from all seismic stations into the varible **amp_data**, and the labels for all the seismic traces into the variable **label_data**. <font color = red>**(4 points)**</font> <br>\n<br>\n**HINT**: Please refer back to the last lab exercise to see how to import data. Please note that in the last lab exericse, I used the variable **flag_data** for all the labels, but here we are going to use a different variable name, **label_data**. <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 3\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 4:**</font> Check the shape of the variable **amp_data**, and make sure that it is a 12,597 X 651 array. <font color = red>**(2 points)**</font>  Please use the terminology that we discussed in class on Slide 22, and explain what these two numbers, 12,597 and 651, each mean. <font color = red>**(2 points)**</font> <br>\n\n"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 4\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('The total of bad seismic traces is:', len(np.where(label_data==0)[0]))\nprint('The total of good seismic traces is:', len(np.nonzero(label_data)[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Preprocessing data\nThe purpose of preprocessing is to get the data ready for the subsequent analysis or computations. The most common preprocessing step in machine learning is to [standardize features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) by removing the mean and scaling to unit variance, just as what you did for your lab exercises on Logistic Regression and Support Vector Machines. <br>\n<br>\nHowever, as was mentioned in last exercise on Decision Trees, to implement Decision Trees does not require feature scaling or centering at all. RF is simply a large collection of Decision Trees. Therefore, RF does not need the standardizing step, either. <br>\n<br>\nBut, it is still important to randomly shuffle our data, for the reasons explained in the last exercise."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.random.seed(42)\nall_data = np.append(amp_data,label_data,1) # put all the seismic traces and their lables into one matrix.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 5:**</font> Randomly permute the data stored in the variable **all_data** using <font color=blue>**np.random.permutation**</font>, and store the permuted data in a new variable **all_data_permute**. <font color = red>**(5 points)**</font> <br>\n<br>\n**HINT**: If you forget how to do it, please refer back to your lab exercise on Decision Trees. Note the variable name in this notebook is different from last time."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 5\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4. Split data into training and cross-validation sets\nSame as what we did in last exercise using Decision Trees, we are going to use the first 10,000 seismic traces as out training data set, and the rest 2,597 traces as test dat set."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 6:**</font> Create the training data set by assigning the first 10,000 instances and their corresponding labels in **all_data_permute** to new variables, **X_train** and **y_train**, respectively. And similarly, create the validation data set by assigning the remaining instances and their corresponding labels in **all_data_permute** to new variables, **X_validation** and **y_validation**, respectively. <font color = red>**(6 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 6\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that, in Scikit-learn, there is a convenient way of splitting the data by using the [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) module, which is widely used in practice. But for our lab exercises, to keep things consistent, and more importantly, to keep the comparison of the prediction accuracies from different ML algorithms fair, we mannually split the whole set of data into a training and validation set."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5. Import and set up RF classifer\n<font color = red>**Task 7:**</font> Import [**RandomForestClassifier**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from Scikit-Learn. Set up your RandomForestClassifier by setting **n_estimators = 100**, **max_depth = 10**, **random_state = 42**, and **class_weight = 'balanced_subsample'**, and assign this classifier to a new variable **rf_clf**. <font color = red>**(10 points)**</font> <br>\n<br>\n**HINT:** In case you forget how to do it, please refer to the last lecture slide, or the official Scikit-Learn documentation on [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). <br>\n<br>\n**NOTE:** We did not discuss the use of **class_weight** in class. It is used to deal with the situation where the number of samples in one class is much larger than the number of samples in the other class. Let us assume that, there are 999 samples in Class 1 and 1 sample in Class 2. You can imagine that, because of the disproportion, machine learning algorithms will have trouble predicting Class 2. In our case here, we also have the disproportion problem because the number of bad seismic traces is three times the number of good seismic traces. In this case, we can use **class_weight** to balance the decision trees."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 7\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6. Train a RF model\n<font color = red>**Task 8:**</font> Train a RF model using the **training** data set, <font color=blue>**X_train**</font> and <font color=blue>**y_train**</font>, and the classifier, <font color=blue>**rf_clf**</font>, you set up above. <font color = red>**(10 points)**</font> <br>\n<br>\n**HINT**: If you forget how to do it, please refer back to our lecture slides, or the accompanying example notebook *RandomForest_example.ipynb*."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 8\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 7. Evaluation\n<font color = red>**Task 9a:**</font> Make predictions on the <font color=blue>**validation**</font> data set, and assign the predictions to a new variable, **y_pred**. <font color = red>**(5 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 9a\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 9b:**</font> Import **classification_report** from **sklearn.metrics**. Print out the classification report. <font color = red>**(5 points)**</font> <br>\n\n**HINT:** Please refer to the last exercise for how to do it."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Answer to Task 9b\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The expected output should look like\n\n<img src = \"ClassificationReport.PNG\">"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# The following code is based on a modification of the codes in this webpage\n# http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nimportances = rf_clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(10):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(50), importances[indices][:50])\nplt.xticks(range(50), indices, rotation = 90)\nplt.xlim([-1, 50])\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The feature importance plot that you have obtained should look similar to the following one:\n    \n<img src = \"FeatureImportance.png\">"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# The prediction error on training data\n1 - rf_clf.score(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The expected output is:  &nbsp;&nbsp;&nbsp;&nbsp; 0.078200000000000047"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# The prediction error on validation data\n1 - rf_clf.score(X_validation,y_validation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The expected output is:  &nbsp;&nbsp;&nbsp;&nbsp; 0.13708124759337692"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 8. Construct error curves\nSimilar to previous lab exercise on Decision Trees, we are going to construct error curves by training a sequence of Random Forests."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 10a:**</font> Write a few sentences to explain why constucting error curves is important for machine learning. <font color = red>**(5 points)**</font> <br>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(answer to Task 10a:)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_errors = np.zeros(25)\nvalidation_errors = np.zeros(25)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 10b:**</font> We are going to train 25 Random Forests with max_depth ranging from 1 to 25. For each Random Forest, please save its prediction errors on both training and validation data sets to **train_errors** and **validation_errors**, respectively. To make our code look clean, we are going to use a for loop to achieve that. Your task is to finish the following for loop. <font color = red>**(20 points)**</font> <br>"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for idepth in np.arange(1,26):\n    print('Random Forest with max_depth = ', idepth)\n    # step 1: set up your RandomForestClassifier. Hint: max_depth = idepth.\n    rf_clf = \n    # step 2: perform training using training data\n    rf_clf.fit\n    # step 3: make predictions using validation data\n    y_pred = \n    # step 4: save prediction errors to train_errors, and validation_errors\n    train_errors[idepth-1] = \n    validation_errors[idepth-1]= ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"The minimum validation error is : \", min(validation_errors))\nprint(\"The best prediction arracy is:\", 1 - min(validation_errors))\n\nbest_depth = np.argmin(validation_errors) + 1\nprint(\"The minimum validation error (i.e., the best prediction accuracy) occurs when max_depth = \", best_depth)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "max_depth =  np.arange(1,26)\nplt.plot(max_depth,train_errors,'-ro',label=\"training errors\")\nplt.plot(max_depth,validation_errors,'-bo',label=\"validation errors\")\nplt.plot(best_depth,validation_errors[best_depth-1],'gD',label=\"Best Depth\")\n#plt.plot([best_depth,best_depth],[0,validation_errors[best_depth-1]],'g-')\nplt.title('error curves',fontsize=20)\nplt.legend(loc=\"upper right\", fontsize=16)\nplt.xlabel(\"Max_depth\", fontsize=20)\nplt.ylabel(\"Prediction errors\", fontsize=20, rotation=90)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Your error curve plot should look similar to the following one:\n    \n<img src = \"errorcurves_25.png\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 9. Applications of Random Forests to geoscience "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<font color = red>**Task 11:**</font> Do a literature search and look for at least <font color=blue>**two**</font> examples where Random Forests are used to solve some geoscience-related problems. Then, report the source of the information (e.g., URL, DOI, etc.), and summarize each example using a few sentences. <font color = red> **(10 points)**</font>"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(answer to Task 11:)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Acknowledgments\nI would like to thank Ying Zhang for manually labeling all the seismic traces, and Prof. Aibing Li for making this data set available to the students in this class. Ms. Zhang also kindly explained the fundamentals of seismic P-wave receiver functions to me. In addition, I would like to acknowledge Simin Gao, a gradute student in Department of Earth and Atmospheric Sciences at University of Houston, for the for loop that was used to construct the error curves. <br>\n\n<img src = \"photo.png\" width=\"400\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Bonus:\n<font color = red>**Task:**</font> Following what you did before, train 25 Extremely Randomized Trees (a.k.a., Extra-Trees) with max_depth ranging from 1 to 25. For each Extra-Trees, please save its prediction errors on both training and validation data sets to **train_errors** and **validation_errors**, respectively, and plot them up. <font color = red>**(10 points)**</font> <br>\n\n**HINT:** To implement Extra-Trees, you will need to import **ExtraTreesClassifier** class instead of **RandomForestClassifier**."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The errors curves that you have constructed based on Extra-Trees should look similar to the following:\n    \n<img src = \"errorcurves_25_ExtraTrees.png\">"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}